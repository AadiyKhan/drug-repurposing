![Language](https://img.shields.io/badge/Language-Python-blue)
![License](https://img.shields.io/badge/License-MIT-green)

# Drug Repurposing Project

This project leverages computational methods to identify potential new uses for existing drugs, accelerating the discovery of treatments for various diseases. By integrating chemical properties, biological targets, and disease associations, it builds a comprehensive knowledge graph and employs machine learning to predict novel drug-disease indications.

## ğŸ“ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Installation](#-installation)
- [ğŸ“Š Usage](#-usage)
- [ğŸ“‚ Project Structure](#-project-structure)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)
- [ğŸ‘¤ Author](#-author)

## âœ¨ Features

*   **Automated Data Ingestion & Cleaning**: Scripts to fetch, parse, and clean drug and target data from public databases.
*   **Comprehensive Feature Engineering**: Calculates molecular properties, generates chemical fingerprints (e.g., Morgan), and extracts insights from drug descriptions using Natural Language Processing (NLP).
*   **Knowledge Graph Construction**: Assembles a unified knowledge graph connecting drugs, targets, and diseases, enabling sophisticated relationship analysis.
*   **Predictive Candidate Identification**: Utilizes graph-based methods and computational models to predict novel drug-disease repurposing candidates.
*   **Interactive Visualization Dashboard**: Provides a Streamlit-powered web application for exploring discovered repurposing candidates and their associated data.

## ğŸ› ï¸ Tech Stack

*   **Python**: The core language for all data processing, analysis, and model building.
    *   **NumPy**: Essential for high-performance numerical operations, especially with large datasets.
    *   **Pandas**: Used extensively for data manipulation and analysis of structured data.
    *   **Streamlit**: Powers the interactive web dashboard for visualizing results.
    *   **PubChemPy**: Used for programmatic access and retrieval of chemical information from PubChem.
    *   **Altair / PyDeck**: Libraries likely used for generating interactive and visually rich plots and maps within the dashboard.
*   **Express.js / JavaScript**: While not central to the core logic, their presence (e.g., in `pydeck` and `altair` entry points) suggests they are used for interactive web components or underlying visualization frameworks.

## ğŸš€ Installation

Follow these steps to set up the project locally.

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/your-username/drug-repurposing.git
    cd drug-repurposing

    ```

2.  **Set up a Python virtual environment:**

    It is highly recommended to use a virtual environment to manage project dependencies.

    ```bash
    python -m venv .venv

    ```

3.  **Activate the virtual environment:**

    *   **On Windows:**
        ```bash
        .venv\Scripts\activate

        ```
    *   **On macOS/Linux:**
        ```bash
        source .venv/bin/activate

        ```

4.  **Install dependencies:**

    This project uses several Python libraries. Since no `requirements.txt` was detected, you might need to create one or install key packages manually.

    To generate a `requirements.txt` from your active environment (if you have installed packages globally):
    ```bash
    pip freeze > requirements.txt

    ```

    Alternatively, manually install common packages inferred from the project:

    ```bash
    pip install pandas numpy scikit-learn matplotlib seaborn streamlit rdkit networkx pubchempy altair pydeck

    ```

    If `requirements.txt` exists, install dependencies as follows:

    ```bash
    pip install -r requirements.txt

    ```

## ğŸ“Š Usage

The project is structured into several phases, designed to be run sequentially, culminating in a dashboard for interactive exploration.

### Running the Data Processing Pipeline

Navigate through the `phase_X` directories and execute the Python scripts in order.

1.  **Phase 1: Initial Data Preparation**
    ```bash
    python phase_1/1_1_initial_cleaning.py
    python phase_1/1_2_target_normalization.py
    python phase_1/1_3_fetch_smiles.py

    ```

2.  **Phase 2: Feature Engineering**
    ```bash
    python phase_2/2_1_calculate_properties.py
    python phase_2/2_2_generate_fingerprints.py
    python phase_2/2_3_nlp_feature_engineering.py

    ```

3.  **Phase 3: Knowledge Graph & Candidate Prediction**
    ```bash
    python phase_3/3_1_target_enrichment_analysis.py
    python phase_3/3_2_build_kg_nodes.py
    python phase_3/3_3_build_kg_edges.py
    python phase_3/3_4_predict_candidates.py

    ```

4.  **Phase 4: Visualization & Recommendations**
    ```bash
    python phase_4/4_1_visualize_discoveries.py
    python phase_4/4_2_final_recommendation.py

    ```

### Running the Discovery Dashboard

After processing all phases, you can launch the interactive Streamlit dashboard:

```bash
streamlit run phase_5/discovery_dashboard.py

```

This command will open the dashboard in your default web browser, typically at `http://localhost:8501`.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ datset/
â”‚   â”œâ”€â”€ drugbank_cleaned_step1.csv
â”‚   â”œâ”€â”€ drugbank_data.csv
â”‚   â”œâ”€â”€ drugbank_morgan_fingerprints.csv
â”‚   â”œâ”€â”€ drugbank_normalized_targets.csv
â”‚   â”œâ”€â”€ drugbank_small_molecules_features.csv
â”‚   â”œâ”€â”€ drugbank_vocabulary.csv
â”‚   â””â”€â”€ drugbank_with_smiles.csv
â”œâ”€â”€ disease_target_enrichment.csv
â”œâ”€â”€ disgenet_gda_subset.csv
â”œâ”€â”€ drug_repurposing_graph.pkl
â”œâ”€â”€ drug_repurposing_graph_final.pkl
â”œâ”€â”€ drugbank_final_features_with_nlp.csv
â”œâ”€â”€ final_recommendation_shortlist.csv
â”œâ”€â”€ phase_1/
â”‚   â”œâ”€â”€ 1_1_initial_cleaning.py
â”‚   â”œâ”€â”€ 1_2_target_normalization.py
â”‚   â””â”€â”€ 1_3_fetch_smiles.py
â”œâ”€â”€ phase_2/
â”‚   â”œâ”€â”€ 2_1_calculate_properties.py
â”‚   â”œâ”€â”€ 2_2_generate_fingerprints.py
â”‚   â””â”€â”€ 2_3_nlp_feature_engineering.py
â”œâ”€â”€ phase_3/
â”‚   â”œâ”€â”€ 3_1_target_enrichment_analysis.py
â”‚   â”œâ”€â”€ 3_2_build_kg_nodes.py
â”‚   â”œâ”€â”€ 3_3_build_kg_edges.py
â”‚   â””â”€â”€ 3_4_predict_candidates.py
â”œâ”€â”€ phase_4/
â”‚   â”œâ”€â”€ 4_1_visualize_discoveries.py
â”‚   â””â”€â”€ 4_2_final_recommendation.py
â”œâ”€â”€ phase_5/
â”‚   â””â”€â”€ discovery_dashboard.py
â”œâ”€â”€ repurposing_candidates_final.csv
â”œâ”€â”€ repurposing_discovery_map.png
â””â”€â”€ uniprot_target_mapping.pkl

```

*   **`dataset/`**: Contains raw and intermediate datasets used throughout the project.
*   **`phase_1/`**: Scripts for initial data cleaning, normalization, and fetching essential drug information.
*   **`phase_2/`**: Scripts for advanced feature engineering, including molecular property calculation, fingerprint generation, and NLP features.
*   **`phase_3/`**: Focuses on building the knowledge graph, performing target enrichment analysis, and predicting drug candidates.
*   **`phase_4/`**: Scripts for visualizing the discoveries and generating final recommendation lists.
*   **`phase_5/`**: Contains the Streamlit dashboard application for interactive data exploration.
*   **`.csv`, `.pkl`, `.png` files**: Various output files from different processing stages, including the final recommendations and visualizations.

## âš™ï¸ Configuration

No specific configuration files (e.g., `.env`, `config.ini`) were detected. All configurations, such as API keys or file paths, are currently embedded within the Python scripts. For customization, you will need to modify the relevant script files directly.

## ğŸ¤ Contributing

Contributions are welcome! If you have suggestions for improving the project, please open an issue or submit a pull request.

## ğŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

## ğŸ‘¤ Author

[Aadiy Khan / student @VIT Bhopal University]
[Ghaziah Shoeb / student @VIT Bhopal University]
